{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for classification and spelling\n",
    "\n",
    "This notebook will enable the user to:\n",
    "1. Train up a classifier on neural data from BRAVO1 during mimed speech attempts, then make predictions on held out test data, as well as calculate feature salience.\n",
    "2. Use the classifier to make realtime predictions on the sentence spelling data. \n",
    "3. Train an N-gram language model for use with our beam search algorithm.\n",
    "4. Use the language model + beam search to improve the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dependencies\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from data_wrangling import load_data , split_data\n",
    "from machine_learning import run_standard_model, prediction_only\n",
    "from bravo_ml.model_eval.torch_salience import get_saliences\n",
    "from saving_functions import save_saliences, make_dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments. This defines the arguments that you can modify, should you want to. \n",
    "#### The necessary arguments to train the basic model are all provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default=None, type=str, \n",
    "                    help='How the data is saved. Should be  \\\n",
    "                    Should be in format %s_restofname.npy for X, Y, and blx (Neural, labels, blocks)')\n",
    "\n",
    "parser.add_argument('--date_dict', default=None, type=str,\n",
    "                   help='A dictionary that maps the block numbers to the date they were recorded')\n",
    "\n",
    "parser.add_argument('--end_date', default=None, type=str, \n",
    "                   help='last day of data to use, YYYY_MM_DD format')\n",
    "\n",
    "parser.add_argument('--device', default='cpu', type=str, \n",
    "                   help='cpu for cpu, cuda to use GPU')\n",
    "\n",
    "parser.add_argument('--model_saving_fp', default=None, help='where to store the models + the base name, \\\n",
    "                                            This should be in the format filepath/<modelname>_%d.pth \\\n",
    "                                            so that we can store models for each CV if you want')\n",
    "\n",
    "parser.add_argument('--feats_to_use', default='hga_and_lfs', type=str, help='which features to use \\\n",
    "                        defaults to both. other options are hga, or lfs')\n",
    "\n",
    "parser.add_argument('--data_dir', default = './data', help='where data is stored')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--balance_classes', action='store_true', help='equal number of classes per category')\n",
    "\n",
    "\n",
    "#### The rest of th parameters shouldn't be changed here, but can be useful for running experiments.\n",
    "parser.add_argument('--start_date', default=None, type=str, \n",
    "                   help='first day of data to use, YYYY_MM_DD format')\n",
    "parser.add_argument('--data_split_scheme', default='random', type=str,\n",
    "                   help='how to split up the data')\n",
    "parser.add_argument('--load_pretrained_model', default=None, type=str, help='path of pretrained models to use for training. '\n",
    "            'Useful for finetuning models')\n",
    "parser.add_argument('--num_cvs', default=10, type=int, help='how many CVs to run')\n",
    "parser.add_argument('--num_samples', default=None, type=int, help='max no. of samples to use for training');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is quite large, it may be useful to load only a small portion of it to enable faster training, and to enable training on RAMs with < 32GB of memory. You can adjust the percentage of the training data to use using this argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--training_data_pct', default=0.5, type=float,  \n",
    "                   help='What percentage of  the data to use. This enables training on smaller RAM CPUs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reduce the size of the dataset, the predictions on the realtime data will be worse. \n",
    "To get better predictions, you can load a model pretrained on all the data up to the day before our realtime data was recorded using the argument below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--load_pretr_for_spell', action='store_true', help='load the pretrained model on full ds for spelling');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments for training the model on a small portion of our dataset\n",
    "This should run on most computers with 16GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the settings we need \n",
    "exp_str = '--end_date 2021_07_20' # Use data collected prior to July 20th [ensures the data is only isolated trial data]\n",
    "# Change this to wherever your data is stored. But if you follow readme instructions, it should be stored in these folders\n",
    "# already. \n",
    "exp_str += ' --dataset %s_alpha_new_mimed.npy'  #\n",
    "exp_str += ' --date_dict date_dictionary_mimed.pkl'\n",
    "exp_str += ' --model_saving_fp ./model_checkpoints/demo_model_partial_data_%d.pth'\n",
    "exp_str += ' --device cpu' # Change to CUDA to use a GPU for training if you installed pytorch appropriately. \n",
    "exp_str += ' --balance_classes'\n",
    "exp_str += ' --data_dir ./data/'\n",
    "exp_str += ' --load_pretr_for_spell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the cell below to change the arguments to those that replicate our pretrained model\n",
    "This requires at least a 32 GB of RAM since we're using a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_str = '--end_date 2021_08_10' # Use data collected prior to August 10th (the session prior to realtime decoding we will use)\n",
    "# exp_str += ' --dataset %s_alpha_new_mimed.npy'  #\n",
    "# exp_str += ' --date_dict date_dictionary_mimed.pkl'\n",
    "# exp_str += ' --model_saving_fp ./model_checkpoints/demo_model_%d.pth'\n",
    "# exp_str += ' --data_dir ./data/'\n",
    "# exp_str += ' --training_data_pct 1.0'\n",
    "# exp_str += ' --device cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the arguments\n",
    "args = vars(parser.parse_args(exp_str.split()))\n",
    "# see the arguments\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the neural data, labels, and block numbers\n",
    "\n",
    "The data should be in 3 files: \n",
    "\n",
    "X_alpha_new_mimed: The neural samples for each trial in a tensor that is N_trials x Timesteps x Channels. Channels 0-127 are the LFS activity from every channel. The other 128 are the HGA activity from each channel. Each sample is the neural activity from 2s prior to the go cue, to 4s after. The data has already been downsampled to 33.33Hz, and the keras.utils.normalize function has been applied. Using the appropriate timewindow is taken care of by a data augmentation in the model training code. \n",
    "\n",
    "blx_alpha_new_mimed: The associated block number for each trial, these are used to map each sample to the day it was recorded. This requires a date_dictionary in the args, which maps from these block numbers to the dates that they were recorded.\n",
    "\n",
    "Y_alpha_new_mimed: The associated label for each trial. Labels 0-25 correspond to A-Z. Label 26 is the motor command.\n",
    "\n",
    "This data is already saved with our decimation factor of 6 and normalization applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['test_end_date'] = args['end_date']\n",
    "\n",
    "# Step 1: Load in the dataset.\n",
    "X, Y, blocks, dates = load_data(join(args['data_dir'], args['dataset']), args['start_date'], args['end_date'], \n",
    "                               join(args['data_dir'], args['date_dict']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_back_amt = -1*X.shape[0]*args['training_data_pct']\n",
    "go_back_amt = int(go_back_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, blocks, dates = X[go_back_amt:], Y[go_back_amt:], blocks[go_back_amt:], dates[go_back_amt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import balance_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['balance_classes']:\n",
    "    X, Y, blocks, dates = balance_classes(X, Y, blocks, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= np.arange(X.shape[1])*6/200 - 2\n",
    "trial = 100\n",
    "elec = 0\n",
    "plt.plot(t, X[trial, :, elec], label='raw data, elec 0')\n",
    "plt.plot(t, X[trial, :, elec+128], label='hga data, elec 0')\n",
    "plt.axvline(0, label='cue onset', c= 'r')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('neural activity (a.u.)')\n",
    "plt.legend()\n",
    "plt.title(\"Neural activity from a single trial, with decimation and normalization applied\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try out testing the model with just high gamma or just raw data to see the effect on accuracy changes (see fig 3). \n",
    "# Note the model will then not work for the sentence predictions later in the notebook.\n",
    "if args['feats_to_use'] == 'lfs':\n",
    "    X = X[:, :, :128]\n",
    "elif args['feats_to_use'] == 'hga': \n",
    "    X = X[:, :, 128:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train the neural network model [just on the mimed dataset]\n",
    "\n",
    "There are two differences between this network and the one used in our evaluation: \n",
    "\n",
    "Firstly, the one we used was pretrained on overt data, then fine tuned on mimed. See fig 4 for more details, but this only minorly improved performance and doubles training time. \n",
    "Secondly, we used model ensembling to average the predictions of 10 models. To save training time for this example, we only train on mimed data, and evaluate using one model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to save all the meaningful metrics we want\n",
    "pred_list = []\n",
    "label_list = []\n",
    "cvs_list = []\n",
    "blocks_list = []\n",
    "prev_sal_start = 0 \n",
    "# all_saliences = np.zeros_like(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will train up the model. \n",
    "It takes about 13-14 epochs for our model to reach around 55% accuracy on the validation set. Keep in mind we're not using all the 9k samples we had, so it will be a little worse. Each epoch should take around a minute. Our final accuracy on the heldout test set should be around 50-60%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the cvs necessary. \n",
    "for cv in range(args['num_cvs']):\n",
    "                \n",
    "    # Use the 2nd CV fold if we're not using all the data. Gives slightly better performance :D \n",
    "    if args['training_data_pct'] != 1.0: \n",
    "        cv = 2\n",
    "    \n",
    "    # Get training and test blocks\n",
    "    X_tr, Y_tr, blocks_tr, X_te, Y_te, blocks_te = split_data((X,Y, blocks), None,\n",
    "                                                              args['data_split_scheme'], \n",
    "                                                              args['num_cvs'], cv, \n",
    "                                                              args['num_samples'])\n",
    "    # Split the training set into validation sets.\n",
    "    X_tr, Y_tr, blocks_tr, X_v, Y_v, blocks_v = split_data((X_tr, Y_tr, blocks_tr),\n",
    "                                                          None,  \n",
    "                                                           args['data_split_scheme'], \n",
    "                                                            args['num_cvs'], cv,\n",
    "                                                          None)\n",
    "    \n",
    "    print('Train samples', X_tr.shape[0], 'Validation samples', X_v.shape[0], 'Test samples', X_te.shape[0])\n",
    "        \n",
    "    # Load the pretrained model if necessary\n",
    "    if not args['load_pretrained_model'] is None: \n",
    "        pretr_model = args['load_pretrained_model']\n",
    "       \n",
    "    else: \n",
    "        pretr_model = None\n",
    "    \n",
    "    \n",
    "    if pretr_model is None: \n",
    "        lr = 1e-3\n",
    "    else: \n",
    "        lr = 1e-4\n",
    "    # Run the training loop. Get back the labels and predictions, as well as teh trained model \n",
    "    labels, preds, te_loader, model = run_standard_model(X_tr, Y_tr, blocks_tr, \n",
    "                                                         X_te, Y_te, blocks_te,\n",
    "                                                         X_v, Y_v, blocks_v, pretr_model, \n",
    "                                                         lr, cv,\n",
    "                                                         args)\n",
    "    \n",
    "    \n",
    "#     # If you run this for more CVs, this code lets you store all the \n",
    "#     # predictions and labels across each CV. \n",
    "#     pred_list.extend(preds)\n",
    "#     label_list.extend(labels)\n",
    "#     cvs_list.extend([cv]*len(labels))\n",
    "#     blocks_list.extend(blocks_te)\n",
    "\n",
    "    \n",
    "    break # we only care about getting one model for now, but to run experiments etc. you can run 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model training complete. The top-1 accuracy was {100*np.mean([np.argmax(p) for p in preds]==labels):.3f}%' , 'on the held out test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load data and metadata from a sentence spelling block. \n",
    "\n",
    "Here we will get the chunks of the data that occur every 2.5 seconds relative to the first go cue, and apply the model. Then we will save the probability vectors from each prediction for use during the beam search. \n",
    "\n",
    "We will also save the most likely character at each timepoint to display the greedy prediction. Because the spaces are added in during the beam search, the greedy decode will not contain spaces.\n",
    "\n",
    "We will also plot a matrix that shows the prediction vector for every timepoint. You can see that the character often still assigns some probability to the correct character when it is wrong, which means a language model can improve the decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports relevant to this setion\n",
    "from scipy.signal import decimate\n",
    "from machine_learning import normalize\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import fastwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args['load_pretr_for_spell']:\n",
    "    from machine_learning import get_model_shell\n",
    "    model = get_model_shell(X_te, Y_te, blocks_te, 0, args)\n",
    "    model.load_state_dict(torch.load('./model_checkpoints/demo_model_0.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = 2726 # The realtime block we're testing on. It was collected on 8/12/21, so none of our training data was from this day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the provided data\n",
    "with open(args['data_dir'] + '/realtime_spelling/block_%d_timing.pkl' %block,'rb') as f: \n",
    "    sentence_to_start = pickle.load(f)\n",
    "\n",
    "# A single timecourse with all of the neural data in it    \n",
    "neural = np.load(args['data_dir'] + '/realtime_spelling/block_%d_neural.npy' %block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character set. \n",
    "characters = 'abcdefghijklmnopqrstuvwxyz$'\n",
    "# put model in eval set\n",
    "model = model.to(args['device']).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Perform \"greedy\" spelling without full language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_realtime_prediction(neural_slice, model): \n",
    "    \"\"\"\n",
    "    Input: Array of neural data at 200 Hz \n",
    "    Outputs: The models prediction given that neural data\n",
    "\n",
    "    \"\"\"\n",
    "    neural_slice = np.expand_dims(neural_slice, axis=0)\n",
    "    # We need to decimate the data since its at 200Hz, and the model operates at 33.33\n",
    "    neural_slice = decimate(neural_slice, 6, axis=1)\n",
    "\n",
    "    # Normalize the data, normalize raw and LFS separately. \n",
    "    # Note the order changes, since there was a small discrepancy in the order of the\n",
    "    # feature streams for the realtime data vs the order used for model training. This\n",
    "    # fixes it. \n",
    "    new_data= np.zeros_like(neural_slice)\n",
    "    new_data[:, :, 128:] = normalize(neural_slice[:, :, :128])\n",
    "    new_data[:, :, :128] = normalize(neural_slice[:, :, 128:])\n",
    "\n",
    "    # Predict using the model we just trained :D \n",
    "    with torch.no_grad():\n",
    "        pred_vec = model(torch.from_numpy(new_data).float().to(args['device']))\n",
    "        pred_vec = F.softmax(pred_vec, dim=-1)\n",
    "    pred_vec = pred_vec.cpu().squeeze().numpy()\n",
    "\n",
    "    return pred_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arrs = []\n",
    "cers = []\n",
    "cue_time_diff = 2.5 # Time between cues in seconds\n",
    "sr = 200 # The original sampling rate of the data. \n",
    "\n",
    "for sent, start in sentence_to_start.items(): \n",
    "    sent += '$' # representing the motor command\n",
    "    greedy = ''\n",
    "    preds = []\n",
    "    for k, char in enumerate(sent.replace(' ', '')): \n",
    "        \n",
    "        \n",
    "        # Get the neural slice out.\n",
    "        char_start_time = k*cue_time_diff + start\n",
    "        char_end_time = char_start_time + cue_time_diff\n",
    "        char_start_ind = int(char_start_time*sr)\n",
    "        char_stop_ind = int(char_end_time*sr)\n",
    "        neural_slice = neural[char_start_ind:char_stop_ind]\n",
    "\n",
    "        # Make the prediction the model would make\n",
    "        pred_vec = simulate_realtime_prediction(neural_slice, model)\n",
    "\n",
    "        # If the motor command didnt have greater than .8 probability, \n",
    "        # then store the prediction over the NATO codeword characters for the beam search.\n",
    "        # Otherwise, stop the process.\n",
    "        if pred_vec[-1] > .8:\n",
    "            print('motor command decoded, system stopped')\n",
    "            break\n",
    "        else:\n",
    "            preds.append(pred_vec[:-1])\n",
    "            \n",
    "            # Store the most likely character\n",
    "            greedy += characters[np.argmax(pred_vec)]\n",
    "            \n",
    "    # Print out the ground truth\n",
    "    sent = sent[:-1] # Remove the Motor command now\n",
    "    print('Ground truth:', sent)\n",
    "    \n",
    "    # print the greedy prediction\n",
    "    print('Greedy prediction:', greedy)\n",
    "    \n",
    "    # Compare how many characters it got right with the sentence without spaces. \n",
    "    wer, cer = fastwer.score([sent.replace(' ', '')],[greedy]), fastwer.score([sent.replace(' ', '')],[greedy],  char_level=True)\n",
    "    print('Greedy CER %.3f:' %cer)\n",
    "   \n",
    "    \n",
    "    # Append predictions to the list of predicted arrays. \n",
    "    pred_arrs.append(np.array(preds))\n",
    "    plt.imshow((np.array(preds).T), cmap='Blues')\n",
    "    plt.yticks(np.arange(26), characters[:-1])\n",
    "    plt.xticks(np.arange(len(preds)), list(sent.replace(' ', '')))\n",
    "    plt.xlabel('Intended Character')\n",
    "    plt.ylabel('Decoder Probability of Each Char. Codeword')\n",
    "    plt.show()\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plug our vectors into our full beam search and language model. This will result in receiving the predictions with spaces inserted and with langauge modeling applied. \n",
    "\n",
    "This should greatly improve the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_models.train_ngram import get_ngram_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# Input arguments\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--vocab_filepath', default=None, type=str, help='where the new vocab is located. list of all the words')\n",
    "parser.add_argument('--ngram_filepath', default=None, type=str, help='where to save the ngram model. end in .pkl') \n",
    "parser.add_argument('--ngram_corpus', default=None, type=str, help='path to the Ngram corpora for training')\n",
    "parser.add_argument('--greedy', action='store_true', help='whether to use the weird greedy beam search or not')\n",
    "parser.add_argument('--paradigm', default='mimed', type=str, help='use mimed or overt blocks')\n",
    "\n",
    "\n",
    "# Language model + beam search parameters. \n",
    "parser.add_argument('--alpha', type=float, default=0.642)\n",
    "parser.add_argument('--beta', type=float, default=10.524)\n",
    "parser.add_argument('--final_lm_alpha', type=float, default=1.5268)\n",
    "parser.add_argument('--beam_width', type=int, default=256)\n",
    "\n",
    "# This will override the previously specified parameters, and use the parameters that were used in realtime during that block. \n",
    "parser.add_argument('--useRTparam', action='store_true', help='use the same params we used in RT')\n",
    "\n",
    "\n",
    "# Respect certain parameters in rt. Not used here, but can be useful for running experiments to change alpha and beta to 0 (e.g. not use LM)\n",
    "parser.add_argument('--hardset_beams', action='store_true', help='lock beam sizes, otherwise they will update \\\n",
    "                    across the demo days')\n",
    "parser.add_argument('--hardset_lms', action='store_true', help='lock lm parameters, otherwise they will update \\\n",
    "                    across teh demo days')\n",
    "parser.add_argument('--greedy_flag', action='store_true', help='greedy decoding');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_str = '--vocab_filepath ./language_models/word_vocab_1k.lex' # The vocabulary you want. To adjust vocabulary size, just use a\n",
    "                                                                # vocabulary with more words. \n",
    "exp_str += ' --ngram_filepath ./language_models/1k_ngram.pkl' # Where to save the LM\n",
    "exp_str += ' --ngram_corpus ./language_models/corpora/' # Contains text files that will be processed to train LM. \n",
    "\n",
    "exp_str += ' --useRTparam' # Use the beam search parameters that we used for this block in realtime. \n",
    "args_lm = vars(parser.parse_args(exp_str.split()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Ngram language model (LM) that we're going to use during the beam search\n",
    "This will take between 3-5 minutes on a macbook pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the vocab/n_gram sorted\n",
    "ngram_lm_fp = get_ngram_lm(args_lm['vocab_filepath'], \n",
    "                           args_lm['ngram_filepath'], \n",
    "                           args_lm['ngram_corpus'])\n",
    "\n",
    "\n",
    "from language_models.autocomplete import * \n",
    "if ngram_lm_fp is None: \n",
    "    ngram_lm_fp = args_lm['ngram_filepath']\n",
    "   \n",
    "# Load the language models\n",
    "lm = load_autocomplete(ngram_lm_fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the search. This will output the most likely sentence at each timepoint, which was provided to the participant as feedback. \n",
    "# It will also show the final point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentences and predictions paired\n",
    "sent_and_preds =[]\n",
    "for sent, pred_arr in zip(sentence_to_start.keys(), pred_arrs):\n",
    "    sent_and_preds.append((sent, pred_arr))\n",
    "\n",
    "\n",
    "# Load the language model parameters\n",
    "lm_config = {\n",
    "    'alpha':args_lm['alpha'],\n",
    "    'beta':args_lm['beta'],\n",
    "    'final_lm_alpha':args_lm['final_lm_alpha'],\n",
    "    'beam_width':args_lm['beam_width']\n",
    "} \n",
    "\n",
    "# Run the simulation. \n",
    "from beam_search_simulation.run_simulation import run_prefix_search\n",
    "\n",
    "gts, preds = run_prefix_search(sent_and_preds, \n",
    "                               lm_config, \n",
    "                               lm, \n",
    "                             [block]*len(sent_and_preds),\n",
    "                               args_lm['vocab_filepath'],  \n",
    "                               useRTparams=args_lm['useRTparam'], \n",
    "                              hardset_lms=args_lm['hardset_lms'], \n",
    "                              hardset_beams=args_lm['hardset_beams'],\n",
    "                              greedy_flag= args_lm['greedy_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the ground-truth sentences\n",
    "gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the predicted sentences\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the primary performance metrics (word and character error rates). The CER should be low. \n",
    "import fastwer\n",
    "wer = fastwer.score(preds, gts)\n",
    "cer = fastwer.score(preds, gts, char_level=True)\n",
    "print('block wer %.2f' %(wer), '%')\n",
    "print('block cer %.2f' %(cer), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
