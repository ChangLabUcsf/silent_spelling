{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5 - Differences in neural signals and classification performance between overt- and silent-speech attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "from silent_spelling.utils import plotting_defaults, holm_bonferroni_correction, bootstrap_confidence_intervals\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject\n",
    "subject = 'bravo1'\n",
    "sr = 200\n",
    "sig_thresh = 0.01\n",
    "pvalue_thresholds = [[1e-4, \"***\"], [0.001, \"**\"], [0.01, \"*\"], [1, \"ns\"]]\n",
    "\n",
    "fig_dir = 'saved_figures'\n",
    "load_from_RT = False\n",
    "save_to_excel = True\n",
    "\n",
    "# Name of the folder that contains result .pkl's\n",
    "result_folder_name = 'spelling_paper_signal_analyses'\n",
    "\n",
    "# Define the result file nums\n",
    "result_nums = {\n",
    "    'alphabet1_2_ecog_trials': 51,\n",
    "    'overt_mimed_acc': 84\n",
    "}\n",
    "\n",
    "training_schemes = {\n",
    "    'mimed': 'Silent only',\n",
    "    'overt': 'Overt only',\n",
    "    'mimed-> ft overt' : 'Silent pre-train,\\novert fine-tune',\n",
    "    'overt-> ft mimed' : 'Overt pre-train,\\nsilent fine-tune',\n",
    "}\n",
    "\n",
    "# Load brain image and electrode coordinates\n",
    "brain_img = plt.imread(f'recon/{subject}_brain_2D.png')\n",
    "elec_coords = np.load(f'recon/{subject}_elecmat_2D.npy')\n",
    "elec_layout = np.load(f'recon/{subject}_elec_layout.npy')\n",
    "\n",
    "paradigms = ['overt', 'mimed']\n",
    "paradigm_names = {\n",
    "    'overt': 'overt',\n",
    "    'mimed': 'silent'\n",
    "}\n",
    "    \n",
    "erp_elecs = [0, 101]\n",
    "erp_words = ['kilo', 'tango']\n",
    "alphabet1_2_ecog_window = np.array([-1, 3])\n",
    "\n",
    "excel_filepath = os.path.join(os.path.split(os.getcwd())[0], 'source_data', 'source_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_RT:\n",
    "    \n",
    "    # Custom software for file handling on Chang Lab systems\n",
    "    from RT.util import fileHandler, RTConfig\n",
    "    \n",
    "    ## Accuracy\n",
    "    result_path = fileHandler.getSubResultFilePath(\n",
    "        sub_dir_key='analysis',\n",
    "        result_label=result_folder_name,\n",
    "        sub_result_num=result_nums['overt_mimed_acc']\n",
    "    )\n",
    "    overt_mimed_acc_df = pd.read_hdf(result_path)\n",
    "    \n",
    "    ## ERP\n",
    "    result_path = fileHandler.getSubResultFilePath(\n",
    "        sub_dir_key='analysis',\n",
    "        result_label=result_folder_name,\n",
    "        sub_result_num=result_nums['alphabet1_2_ecog_trials']\n",
    "    )\n",
    "    with open(result_path, 'rb') as f:\n",
    "        alphabet1_2_ecog = pickle.load(f)\n",
    "        \n",
    "    # columns\n",
    "    erp_dict = {}\n",
    "    for para in paradigms:\n",
    "\n",
    "        erp_dict[para] = {}\n",
    "        for word in erp_words:\n",
    "\n",
    "            erp_dict[para][word] = {}\n",
    "            for elec in erp_elecs:\n",
    "\n",
    "                idx = np.where(alphabet1_2_ecog['labels'][para] == word)[0]\n",
    "                erp_dict[para][word][elec] = alphabet1_2_ecog['ecog'][para][idx, :, elec]\n",
    "                \n",
    "    if save_to_excel:\n",
    "        \n",
    "        if os.path.exists(excel_filepath):\n",
    "            mode = 'a'\n",
    "        else:\n",
    "            mode = 'w'\n",
    "        \n",
    "        with pd.ExcelWriter(excel_filepath, mode=mode) as writer:  \n",
    "            \n",
    "            for para in paradigms:\n",
    "                for word in erp_words:\n",
    "                    for elec in erp_elecs:\n",
    "                        pd.DataFrame(erp_dict[para][word][elec]).to_excel(\n",
    "                            writer, sheet_name=f'Fig 5BC_{para}_{word}_{elec}', index=False\n",
    "                        )\n",
    "            \n",
    "            overt_mimed_acc_df.to_excel(writer, sheet_name='Fig 5D', index=False)\n",
    "\n",
    "else:\n",
    "    \n",
    "    erp_dict = {}\n",
    "    for para in paradigms:\n",
    "\n",
    "        erp_dict[para] = {}\n",
    "        for word in erp_words:\n",
    "\n",
    "            erp_dict[para][word] = {}\n",
    "            for elec in erp_elecs:\n",
    "\n",
    "                data = pd.read_excel(excel_filepath, sheet_name=f'Fig 5BC_{para}_{word}_{elec}', engine='openpyxl')\n",
    "                erp_dict[para][word][elec] = data.values\n",
    "                \n",
    "    overt_mimed_acc_df = pd.read_excel(excel_filepath, sheet_name='Fig 5D', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overt vs silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `overt` vs `silent`, decoding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overt_mimed_acc_df['unique scheme'] = [f'{i}${j}' for i, j in zip(overt_mimed_acc_df['training scheme'].values, overt_mimed_acc_df['test data'].values)]\n",
    "\n",
    "# Perform statistical tests\n",
    "pvals_float = {}\n",
    "stats_float = {}\n",
    "for c1, c2 in itertools.combinations(overt_mimed_acc_df['unique scheme'].unique(), 2):\n",
    "    key = f'{c1}&{c2}'\n",
    "    group1 = overt_mimed_acc_df.loc[overt_mimed_acc_df['unique scheme'] == c1]['accuracy'].values\n",
    "    group2 = overt_mimed_acc_df.loc[overt_mimed_acc_df['unique scheme'] == c2]['accuracy'].values\n",
    "    stats_float[key], pvals_float[key] = stats.ranksums(group1, group2)\n",
    "\n",
    "# Holm-Bonferroni correction\n",
    "hbc_pval_str = {}\n",
    "hbc_pvals = holm_bonferroni_correction(pvals_float)\n",
    "print(f'{len(pvals_float.keys())}-way Holm-Bonferroni correction')\n",
    "    \n",
    "# Format for stat annot\n",
    "acc_box_pairs, acc_pvals = [], []\n",
    "for key, val in hbc_pvals.items():\n",
    "    \n",
    "    if val > sig_thresh:\n",
    "        print(key.split('&'), val, 'not significant')\n",
    "        \n",
    "    else:\n",
    "        print(key.split('&'), val)\n",
    "        continue\n",
    "        \n",
    "    c1, c2 = key.split('&')\n",
    "    acc_box_pairs.append((tuple(c1.split('$')), tuple(c2.split('$'))))\n",
    "    acc_pvals.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print output for Latex table (Supplementary Table S4)\n",
    "\n",
    "for key, hbpval in hbc_pvals.items():\n",
    "    group1, group2 = key.split('&')\n",
    "    train1, test1 = group1.split('$')\n",
    "    train2, test2 = group2.split('$')\n",
    "    \n",
    "    if '-> ft' in train1:\n",
    "#         train1 = train1.replace('-> ft', ' pre-train, \\\\\\\\')\n",
    "        train1 = train1.replace('-> ft', ' pre-train,')\n",
    "        train1 += ' fine-tune'\n",
    "        \n",
    "    if '-> ft' in train2:\n",
    "#         train2 = train2.replace('-> ft', ' pre-train, \\\\\\\\')\n",
    "        train2 = train2.replace('-> ft', ' pre-train,')\n",
    "        train2 += ' fine-tune'\n",
    "        \n",
    "    train1 = \"\\\\thead{\" + train1.capitalize() + \"}\"\n",
    "    train2 = \"\\\\thead{\" + train2.capitalize() + \"}\"\n",
    "    test1 = \"\\\\thead{\" + test1.capitalize() + \"}\"\n",
    "    test2 = \"\\\\thead{\" + test2.capitalize() + \"}\"\n",
    "    \n",
    "#     train1 = train1.capitalize()\n",
    "#     train2 = train2.capitalize()\n",
    "#     test1 = test1.capitalize()\n",
    "#     test2 = test2.capitalize()\n",
    "    \n",
    "    string = ' & '.join([train1, test1, train2, test2, str(abs(np.round_(stats_float[key], 2))), \n",
    "                         '\\\\num{' + str(np.format_float_scientific(hbpval, precision=2)) + '}'])\n",
    "    string += \"  \\\\\\\\\"\n",
    "    string = string.replace('mimed', 'silent').replace('Mimed', 'Silent')\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unique_scheme in overt_mimed_acc_df['unique scheme'].unique():\n",
    "    b = overt_mimed_acc_df.loc[overt_mimed_acc_df['unique scheme'] == unique_scheme]['accuracy'].values\n",
    "    print(unique_scheme.split('$'), 100*np.median(b), 100*bootstrap_confidence_intervals(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dark2 colors\n",
    "set2_colors = sns.color_palette(\"Set2\")\n",
    "\n",
    "colors = {\n",
    "    'overt': set2_colors[0],\n",
    "    'mimed': set2_colors[1],\n",
    "    'alphabet1_1': set2_colors[4],\n",
    "    'alphabet1_2': set2_colors[6],\n",
    "    'hga': set2_colors[3],\n",
    "    'raw': set2_colors[2],\n",
    "    'hga + raw': set2_colors[7]\n",
    "}\n",
    "\n",
    "brain_closeup = {\n",
    "    'xlim': [200, 500],\n",
    "    'ylim': [150, 550]\n",
    "}\n",
    "\n",
    "set2_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linewidth = 1\n",
    "annot_linewidth = 1\n",
    "fontsize = 7\n",
    "plotting_defaults(font='Arial', fontsize=fontsize, linewidth=linewidth)\n",
    "panel_label_fontsize = 7\n",
    "boxplot_kwargs = {'fliersize': 3}\n",
    "scatter_size = 3\n",
    "mm = 1 / 25.4\n",
    "mm_figsize = [mm*180, mm*120]\n",
    "\n",
    "fig = plt.figure(figsize=mm_figsize, constrained_layout=True)\n",
    "\n",
    "# fig = plt.figure(figsize=(15, 10), constrained_layout=True)\n",
    "gs = mpl.gridspec.GridSpec(4, 6, figure=fig)\n",
    "axs = {}\n",
    "\n",
    "\n",
    "##### ----- Brain plot\n",
    "axs['brain'] = fig.add_subplot(gs[:2, :2])\n",
    "axs['brain'].imshow(brain_img, alpha=0.2)\n",
    "axs['brain'].axis('off')\n",
    "\n",
    "for i in range(elec_coords.shape[0]):\n",
    "    if i in erp_elecs:\n",
    "        axs['brain'].scatter(elec_coords[i, 0], elec_coords[i, 1], color='k', s=scatter_size+5)\n",
    "        axs['brain'].annotate(f'e{i}', (elec_coords[i, 0] + 15, elec_coords[i, 1]))\n",
    "    else:\n",
    "        axs['brain'].scatter(elec_coords[i, 0], elec_coords[i, 1], color='k', s=0.5, alpha=0.5)\n",
    "        \n",
    "##### ----- Make ERP panel\n",
    "erp_ylim = [-0.5, 1.5]\n",
    "erp_yticks = np.arange(-0.5, 1.51, 0.5)\n",
    "\n",
    "axs['erps'] = [fig.add_subplot(gs[0, 2:4]), fig.add_subplot(gs[0, 4:6]), fig.add_subplot(gs[1, 2:4]), fig.add_subplot(gs[1, 4:6])]\n",
    "\n",
    "for ax in axs['erps']:\n",
    "    ax.axvline(x=0.0, linestyle=':', color='k')\n",
    "    ax.axhline(y=0.0, linestyle=':', color='k')\n",
    "    ax.axes.set(xlabel='Time (s)', ylim=erp_ylim, yticks=erp_yticks)\n",
    "    \n",
    "x = np.linspace(alphabet1_2_ecog_window[0], alphabet1_2_ecog_window[1], num=erp_dict['overt'][erp_words[0]][erp_elecs[0]].shape[1])\n",
    "counter = 0\n",
    "for cur_elec, elec in enumerate(erp_elecs):\n",
    "    axs['erps'][cur_elec*2].axes.set(ylabel=f'e{elec} HGA\\n(z-score)')\n",
    "    for cur_word, word in enumerate(erp_words):\n",
    "        axs['erps'][cur_word].axes.set(title=f'\"{word}\"')\n",
    "        \n",
    "        for para in paradigms:\n",
    "            ax = axs['erps'][counter]\n",
    "            sns.despine(ax=ax, offset=dict(bottom=5, left=5))\n",
    "            ax.axes.set(xlim=alphabet1_2_ecog_window)\n",
    "            hga_trials = erp_dict[para][word][elec]\n",
    "            y = hga_trials.mean(0)\n",
    "            err = stats.sem(hga_trials, axis=0)\n",
    "            ax.plot(x, y, label=paradigm_names[para].capitalize(), color=colors[para])\n",
    "            ax.fill_between(x, y - err, y + err, alpha=0.5, color=colors[para])\n",
    "            \n",
    "        counter += 1\n",
    "\n",
    "axs['erps'][1].legend(bbox_to_anchor=(1.05, 1.2), loc='upper right', frameon=False)\n",
    "\n",
    "\n",
    "##### ----- Overt vs mimed accuracies\n",
    "axs['overt_mimed_acc'] = fig.add_subplot(gs[2:, 1:5])\n",
    "axs['overt_mimed_acc'] = sns.boxplot(data=overt_mimed_acc_df, y='accuracy', x='training scheme', hue='test data',\n",
    "                                     palette=[colors['overt'], colors['mimed']], ax=axs['overt_mimed_acc'],\n",
    "                                     hue_order=['overt', 'mimed'], order=training_schemes.keys(),showfliers=False)\n",
    "axs['overt_mimed_acc'] = sns.stripplot(data=overt_mimed_acc_df, y='accuracy', x='training scheme', hue='test data',\n",
    "                                     palette=[colors['overt'], colors['mimed']], ax=axs['overt_mimed_acc'],\n",
    "                                     hue_order=['overt', 'mimed'], order=training_schemes.keys(), \n",
    "                                       edgecolor='black', linewidth=linewidth - 0.5, size=scatter_size, dodge=True)\n",
    "current_handles, current_labels = axs['overt_mimed_acc'].get_legend_handles_labels()\n",
    "axs['overt_mimed_acc'].legend(frameon=True, title=r'$\\bf{Test\\,data}$', loc='upper left',\n",
    "                              handles=current_handles, labels=[l.capitalize() for l in paradigm_names.values()],\n",
    "                             bbox_to_anchor=(1, 1.05))\n",
    "axs['overt_mimed_acc'].axes.set(xlabel='Training scheme', ylabel='Accuracy', ylim=(0.2, 0.7),\n",
    "                                xticklabels=list(training_schemes.values()))\n",
    "axs['overt_mimed_acc'].axhline(y=1/26, linestyle=':', color='k')\n",
    "add_stat_annotation(axs['overt_mimed_acc'], data=overt_mimed_acc_df, y='accuracy', x='training scheme', hue='test data',\n",
    "                    hue_order=['overt', 'mimed'], order=training_schemes.keys(),\n",
    "                    box_pairs=acc_box_pairs, perform_stat_test=False, pvalues=acc_pvals,\n",
    "                    text_format='star', loc='outside', pvalue_thresholds=pvalue_thresholds, \n",
    "                    fontsize='medium', line_offset=0.01, linewidth=annot_linewidth)\n",
    "fig.text(1.02, 1.05, 'P < 0.01 for all\\ncomparisons\\nnot marked ns', ha='left', fontsize=fontsize, transform=axs['overt_mimed_acc'].transAxes)\n",
    "\n",
    "\n",
    "##### ----- Figure panel labels\n",
    "fig.text(0.00, 0.98, r'$\\bf{a}$', ha='left', fontsize=panel_label_fontsize, weight='bold')\n",
    "fig.text(0.29, 0.98, r'$\\bf{b}$', ha='left', fontsize=panel_label_fontsize, weight='bold')\n",
    "fig.text(0.65, 0.98, r'$\\bf{c}$', ha='left', fontsize=panel_label_fontsize, weight='bold')\n",
    "fig.text(0.1, 0.48, r'$\\bf{d}$', ha='left', fontsize=panel_label_fontsize, weight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_dpi = 300\n",
    "\n",
    "for ext in ['png', 'pdf']:\n",
    "    fig.savefig(os.path.join(fig_dir, f'figure5_overt_vs_mimed.{ext}'), \n",
    "                transparent=True, bbox_inches='tight', dpi=figure_dpi)\n",
    "    fig.savefig(os.path.join(fig_dir, f'figure5_overt_vs_mimed_white.{ext}'), \n",
    "                transparent=False, bbox_inches='tight', dpi=figure_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gentz",
   "language": "python",
   "name": "gentz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
